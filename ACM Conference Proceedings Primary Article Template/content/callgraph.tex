\section{Call Graphs}
In 2016, Petrashko et al., introduced context-sensitive call graph construction algorithms that leverage generic type information in object-oriented languages. 
They proposed two extensions to Scala: one that uses actual type arguments for method contexts and another that refines contexts using more precise subtypes from static types of actual arguments. Their approach significantly improved call graph precision and reduced analysis time, demonstrating the value of incorporating generic type information into call graph analyses.

In 2020, Santos et al., introduced Salsa targets Java programs with serialization and deserialization, enhancing existing points-to and call-graph analyses. It employs an on-the-fly, iterative framework that refines call graphs under explicit assumptions about serialization-related behavior. It addresses the challenges of performing static analysis on programs with dynamic features, particularly focusing on serialization and deserialization processes. To regain soundness, Salsa injects synthetic nodes and edges modeling serialization effects into previously computed call graphs and re-analyzes until convergence. Initial results on the Java Call Graph Test Suite (JCG) indicate that Salsa improves call-graph soundness for programs exercising serialization features.

In 2022, Le-Cong et al., introduced AutoPruner, a novel technique for call graph pruning that utilizes both statistical semantic and structural analysis to eliminate false positives in call graphs. Unlike previous machine learning approaches that primarily relied on structural features, AutoPruner employs a Transformer-based model to capture semantic relationships between caller and callee functions. Unlike cgPruner and pure program-analysis methods that consider only the caller, AutoPruner analyzes the source code of both the caller and callee for each edge. It derives semantic embeddings using a pre-trained code model (e.g., CodeBERT), which can learn relationships from paired inputs, and combines them with per-edge structural features. Given a call graph from a static analyzer, AutoPruner preprocesses the graph, extracts these features, and feeds them to a neural classifier that predicts false-positive edges; predicted false positives are pruned to yield a more precise call graph with fewer false alarms.
Empirical evaluations demonstrate that AutoPruner significantly outperforms state-of-the-art methods, achieving up to 13\% improvement in F-measure and reducing false alarm rates in client analyses.

In 2024, Santos et al., introduced Seneca, a call-graph construction technique for Java that makes serialization and deserialization callbacks explicit. By combining taint analysis with API modeling, it resolves callback targets arising during (de-)serialization to produce sound call graphs with minimal overhead. Seneca performs a novel taint-based call graph construction, which relies on the taint state of variables when computing possible dispatches for callback methods.  In evaluation, Seneca identifies vulnerable paths related to untrusted deserialization, passes all serialization-callback tests, and yields fewer spurious edges than Soot and OPAL, while remaining practical in performance.

In 2024, Helm et al., introduced Unimocg, a modular call-graph framework that separates type-information computation from call resolution, enabling interchangeable algorithms and language-feature modules. It supports reuse of type information in downstream analyses, preserves soundness across ten algorithms without precision or performance loss, and outperforms Soot and WALA's emulated RTA.


\todo{Future Directions and Combinations}

Future work should investigate composing modular construction and pruning techniques to further improve efficiency and precision. A practical pipeline could construct call graphs with Unimocg and subsequently prunes edges with AutoPruner. Seneca can also be integrated as the serialization/callback module within Unimocg, followed by AutoPruner to remove residual false positives. 
As an alternative, Salsa's serialization modeling can precede AutoPruner. For Scala codebases, Petrashko et al.'s context-sensitive construction can be paired with AutoPruner. Seneca and Salsa should not be combined in the same pipeline because they target the same (de-)serialization callbacks; one should be selected based on requirements.
